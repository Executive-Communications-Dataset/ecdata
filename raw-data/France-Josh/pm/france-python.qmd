---
title: "French Webscraper"
format: html
jupyter: pyton3       
---

So it looks like I may have to bite the bullet and use python 


```{python}
#| eval: false
pip install selenium
pip install beautifulsoup4
pip install webdriver-manager
pip install lxml
pip install pandas
pip install polars

```


Okay now lets load in everyting 



```{python}
import random
import time
from selenium.webdriver.common.by import  By
import requests 
from lxml import html
from selenium import webdriver
import pandas as pd


global_url = 'https://www.elysee.fr/toutes-les-actualites'

driver = webdriver.Firefox()


driver.get(global_url)



accept_cookies = driver.find_element(By.ID, "tarteaucitronPersonalize2")

accept_cookies.click()

all_articles = driver.find_element(By.ID, 'all-articles-link')

all_articles.click()
                 

```



Okay now we are basically where we are with the R. So we need to grab the hrefs and dates.
Which we can just do with selenium. This will only get one page 



```{python}

get_refs = driver.find_elements(By.CSS_SELECTOR, '.newsBlock-title a')

get_all_refs = [get_refs.get_attribute('href') for get_refs in get_refs]


next_page_selector = '#pagination > div > ul > li.ais-Pagination-item.custom-pagination-item.ais-Pagination-item--nextPage.previous-next.custom-pagination-next--page'

next_page_1 = driver.find_element(By.CSS_SELECTOR, '.custom-pagination-previous--page')


is_disabled = "disabled" in next_page_1.get_attribute("class")

next_page_1.click()


driver.maximize_window()

get_counter_value = next_page.get_attribute("dataValue")

get_dates = driver.find_elements(By.CSS_SELECTOR, '.newsBlock-date')

get_all_dates = [get_dates.get_attribute('textContent') for get_dates in get_dates]


dates_df = pd.DataFrame({"dates": pd.Series(get_all_dates), "urls": pd.Series(get_all_refs)})


```



cool now we can try to iterate this. lets first define the scraping functions 



```{python}

def get_links(driver):
    links_this_page_list = list()
    get_refs = driver.find_elements(By.CSS_SELECTOR, '.newsBlock-title a')
    get_all_refs = [get_refs.get_attribute('href') for get_refs in get_refs]
    links_this_page_list.append(get_all_refs)
    return links_this_page_list


```

Move to next page 

```{python}



def click_next_page(driver):
    next_page = driver.find_element(By.CSS_SELECTOR, '.custom-pagination-next--page')
    next_page.click()
    time.sleep(5)
    return driver

```


```{python}

def get_dates(driver):
    dates_this_page = list()
    get_dates = driver.find_elements(By.CSS_SELECTOR, '.newsBlock-date')
    get_all_dates = [get_dates.get_attribute('textContent') for get_dates in get_dates]
    dates_this_page.append(get_all_dates)
    return dates_this_page


```


Okay so now we need to iterate this thing 




```{python}
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException


links = []

dates = []

is_disabled = False

go_to_first_page = driver.find_element(By.CSS_SELECTOR, '.custom-pagination-first--page')

go_to_first_page.click()

driver.maximize_window()


try:
    while is_disabled == False:  # Continue while is_disabled is False
        links_on_page = get_links(driver)
        dates_on_page = get_dates(driver)
        links.extend(links_on_page)
        dates.extend(dates_on_page)
        time.sleep(15)
        try:
            click_next_page(driver)
            driver.maximize_window()
            next_page_element = driver.find_element(By.CSS_SELECTOR,'.custom-pagination-next--page')
            is_disabled = "disabled" in next_page_element.get_attribute("class")
            print("Next page button is", is_disabled, "Moving to next Page")
            time.sleep(15)
            if is_disabled == True:
                print("Reached Last Page")
                break
        except Exception as e:
            print("An error occurred:", str(e))
finally:
    driver.quit()


            


```




```{python}


links_data = {'url': links, 'dates': dates}


links_df = pd.DataFrame.from_records(links)

links_df.to_csv("links_data.csv")

dates_df = pd.DataFrame.from_records(dates)


dates_df.to_csv("dates_data.csv")








```